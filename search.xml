<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>kafka 学习</title>
    <url>/2020/01/01/kafka-%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h1 id="1-Kafka-基础概念"><a href="#1-Kafka-基础概念" class="headerlink" title="[1] Kafka 基础概念"></a>[1] Kafka 基础概念</h1><p>​    Kafka 起初是由 LinkedIn 公司采用 Scala 语言开发的一个多分区, 多副本且基于 ZooKeeper 协调的分布式消息系统.</p>
<p>Kafka 支持三大功能 :</p>
<ul>
<li><strong>消息系统</strong>：Kafka 和传统消息一样 都具备 系统解耦、冗余存储、 流量消锋、缓冲、异步通信、扩展性、可恢复性等功能。除此之外 Kafka 还支持同分区下消息的顺序性以及回溯消费功能</li>
<li><strong>存储系统</strong>：Kafka 把消息持久化到磁盘，相比其他基于内存存储的系统而言，有效的减低了数据丢失的风险。</li>
<li><strong>流式处理平台</strong>：Kafka 提供了一个完整的流式处理类库</li>
</ul>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>​    Kafka 体系架构包括若干个 Producer、Broker、Conusmer，以及一个 Zookeeper 集群。</p>
<p>​    Producer 将消息发送到 Broker，Broker 负责将受到的消息存储到磁盘中，而 Consumer 负责从 Broker 订阅并消费消息</p>
<p><strong>Zookeeper</strong>：是 Kafka 用来负责集群元数据的管理、控制器的选举等操作。</p>
<p><strong>Producer</strong>：是 生产者，发送消息的一方</p>
<p><strong>Consumer</strong>： 是消费者，接受消息的一方</p>
<p><strong>Broker</strong>：服务代理节点。对于 Kafka 而言，Broker 可以简单的看作一个独立的 Kafka 服务节点或 Kafka 服务实例，一个或多个 Broker 组成一个 Kafka 集群。</p>
<p><strong>Topic</strong>：发送消息前需要先定义一个主题</p>
<p><strong>Position</strong>：分区，在创建 Topic 时可自定分区数量，分区存放消息，利用消息的 Offset 保证 分区内消息的顺序性。也可以在创建完 Topic 后再修改分区数量，实现水平扩展</p>
<p><strong>Offset</strong>：消息在被追加到分区日志文件时会分配一个偏移量 offset</p>
<p><strong>Replica</strong>：为分区引入多副本机制，可以通过增加副本的数量来提升容灾能力。同一分区中不同的副本保存的消息是一致的，副本之间是一主多从关系，leader 副本负责读写请求，follower 副本只负责与 leader 副本消息同步。副本保存在不同的 broker中， 这样可以避免某个 broker 挂掉后仍然能保证服务可用。</p>
<p><strong>AR</strong>：分区中的所有副本统称 Assigend Replicas </p>
<p><strong>ISR</strong>：所有与 leader 副本保持同步的副本（包括 leader ）组成 In-Sync Replicas</p>
<p><strong>OSR</strong>：所有与 leader 副本同步滞后过多的副本（不包括 leader）组成 Out-of-Sync Replicas</p>
<p>默认情况下只有 ISR 集合中的副本才有资格被选举为新的 leader</p>
<p><strong>HW</strong>：High Watermark 它标识一个特定的消息偏移量 offset，消费者只能拉取到这个 offset 之前的消息。</p>
<p><strong>LEO</strong>： LogEndOffset 当前将要插入的最新消息的 offset </p>
<h1 id="2-生产者"><a href="#2-生产者" class="headerlink" title="[2] 生产者"></a>[2] 生产者</h1><h2 id="生产者客户端"><a href="#生产者客户端" class="headerlink" title="生产者客户端"></a>生产者客户端</h2><h3 id="1-引入依赖"><a href="#1-引入依赖" class="headerlink" title="1 引入依赖"></a>1 引入依赖</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.0.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<h3 id="2-配置生产者参数与构建生产者"><a href="#2-配置生产者参数与构建生产者" class="headerlink" title="2 配置生产者参数与构建生产者"></a>2 配置生产者参数与构建生产者</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> KafkaProducer&lt;String,String&gt; <span class="title">initConfig</span><span class="params">()</span></span>&#123;</span><br><span class="line">    Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">    properties.put(<span class="string">"key.serializer"</span>,</span><br><span class="line">                   <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">    properties.put(<span class="string">"value.serializer"</span>,</span><br><span class="line">                   <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">    properties.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">	KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer(properties);</span><br><span class="line">    <span class="keyword">return</span> producer;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-构建待发送消息、发送消息、关闭生产者"><a href="#3-构建待发送消息、发送消息、关闭生产者" class="headerlink" title="3 构建待发送消息、发送消息、关闭生产者"></a>3 构建待发送消息、发送消息、关闭生产者</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    KafkaProducer&lt;String, String&gt; producer = initConfig();</span><br><span class="line">    ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"topic-demo"</span>, <span class="string">"hello!"</span>);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        producer.send(record);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    producer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="必填的参数配置"><a href="#必填的参数配置" class="headerlink" title="必填的参数配置"></a>必填的参数配置</h2><p><strong>bootstrap.servers</strong>：用来自定生产者连接 kafka 集群所需要的 broker 地址，多个则用逗号分割。如果有多个不需要全部列举，但至少填两个以上的 broker ，因为生产者会根据一个 broker 里查找其他的 broker 信息。</p>
<p><strong>key.serializer</strong> 和 <strong>value.serializer</strong>： broker 端接受的消息必须以字节数组（byte[]）的形式存在。这两个参数指定消息的 key 和 value 序列化类型，字符串内容必须是全类名</p>
<p>使用 <strong>ProducerConfig</strong> 代替字符串类型的配置</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"localhost:9092"</span>);</span><br></pre></td></tr></table></figure>



<h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><h3 id="send"><a href="#send" class="headerlink" title="send()"></a>send()</h3><h4 id="1-采用异步发送消息"><a href="#1-采用异步发送消息" class="headerlink" title="1 采用异步发送消息"></a>1 采用异步发送消息</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="comment">// send 方法是异步的</span></span><br><span class="line">producer.send(record);</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h4 id="2-采用同步发送消息"><a href="#2-采用同步发送消息" class="headerlink" title="2 采用同步发送消息"></a>2 采用同步发送消息</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="comment">// send 方法是异步的, 可以链式调用 get() 方法导致同步,返回 metadata 对象</span></span><br><span class="line"><span class="comment">// metadata 保存的是消息的元数据信息:当前消息的主题,分区号,偏移量 offset,时间戳等</span></span><br><span class="line"><span class="comment">// get(long timeout, TimeUnit unit) 实现可超时的阻塞</span></span><br><span class="line">RecodeMetadata metadata = producer.send(record).get();</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h4 id="3-采用异步回调发送消息"><a href="#3-采用异步回调发送消息" class="headerlink" title="3 采用异步回调发送消息"></a>3 采用异步回调发送消息</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">producer.send(record, (metadata, exception) -&gt; &#123;</span><br><span class="line">    <span class="comment">// 发送异常则不为null</span></span><br><span class="line">    <span class="keyword">if</span> (exception != <span class="keyword">null</span>) </span><br><span class="line">        exception.printStackTrace();</span><br><span class="line">    <span class="keyword">else</span> </span><br><span class="line">        System.out.println(metadata.topic() + <span class="string">"-"</span> +</span><br><span class="line">                           metadata.partition() + <span class="string">":"</span> + metadata.offset());</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<h3 id="close"><a href="#close" class="headerlink" title="close()"></a>close()</h3><p>close() 方法会阻塞等待之前所有的发动请求完成后再关闭 kafkaProducer</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// producer.close(long timeout, TimeUnit unit); 可实现带超时时间的</span></span><br><span class="line">producer.close();</span><br></pre></td></tr></table></figure>



<h2 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h2><p>客户端的自带的序列化有：String、ByteBuffer、Bytes、Double、Integer、Long </p>
<p>它们都实现了 <code>org.apache.kafka.common.serialization.Serializer</code> 接口</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 序列化的类都必须实现该接口, 自定义序列化就是实现 Serializer 接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Serializer</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">Closeable</span> </span>&#123;</span><br><span class="line">	<span class="comment">// 用来配置当前类</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs, <span class="keyword">boolean</span> isKey)</span></span>;</span><br><span class="line">	<span class="comment">// 执行序列化操作: 将 data 序列化为 byte[]</span></span><br><span class="line">    <span class="keyword">byte</span>[] serialize(String topic, T data);</span><br><span class="line">	<span class="comment">// 空方法 如果实现了次方法,则必须确保次方法的幂等性,KafakProducer 可能会多次调用</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="分区器"><a href="#分区器" class="headerlink" title="分区器"></a>分区器</h2><p>如果 ProducerRecord 不指定 position 分区 则会使用分区器</p>
<p>默认的分区器 <code>org.apache.kafka.clients.producer.internals.DefaultPartitioner</code></p>
<p>所有分区器都实现 <code>org.apache.kafka.clients.producer.Partitioner</code> 接口</p>
<p><strong>Partitioner</strong> 接口继承 <code>Configurable</code> 类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Partitioner</span> <span class="keyword">extends</span> <span class="title">Configurable</span>, <span class="title">Closeable</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 分区器的具体逻辑</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Configurable</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 可以获取配置信息</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>DefaultPartitioner</strong> 规则如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">如果 指定的 partition 为空, 但是 key 不为空, 则对 key 进行哈希(采用 murmur2 算法) 得到分区号</span><br><span class="line">如果 指定的 partition 为空, key 为空, 消息会轮询的方式发往主题内的各个分区</span><br></pre></td></tr></table></figure>

<h2 id="生产者拦截器"><a href="#生产者拦截器" class="headerlink" title="生产者拦截器"></a>生产者拦截器</h2><p>自定义的拦截器需要实现 <code>org.apache.kafka.clients.producer.ProducerInterceptor</code> 接口</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ProducerInterceptor</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Configurable</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 发送之前</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ProducerRecord&lt;K, V&gt; <span class="title">onSend</span><span class="params">(ProducerRecord&lt;K, V&gt; record)</span></span>;</span><br><span class="line">    <span class="comment">// 发送之后</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onAcknowledgement</span><span class="params">(RecordMetadata metadata, Exception exception)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>拦截器没有默认的, 自定义拦截器写完后需要在 <code>properties</code> 配置</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">properties.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,</span><br><span class="line">               ProducerInterceptorPrefix<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br></pre></td></tr></table></figure>



<h2 id="2-7-原理分析"><a href="#2-7-原理分析" class="headerlink" title="2.7 原理分析"></a>2.7 原理分析</h2><h3 id="生产者客户端的整体架构："><a href="#生产者客户端的整体架构：" class="headerlink" title="生产者客户端的整体架构："></a>生产者客户端的整体架构：</h3><p><img src="../images/producer.PNG" alt="生产者客户端的整体架构"></p>
<p><strong>RecordAccumulator</strong>：创建后的消息最终进入消息累加器缓存，一定的时间过后 Sender 线程批量发送，进而减少网络传输的资源消耗。缓存的大小通过 <code>buffer.memory</code> 配置，默认 33554432B 即 32MB。如果发送到累加器的速度比发送到服务器的速度快，则 Send() 要么被阻塞，要么爬出异常，取决于参数 <code>max.block.ms</code> 的配置，默认 60000 ，即 60秒</p>
<p><strong>ProducerBatch</strong>：存放一个或多个 ProducerRecord 。</p>
<p>org.apache.kafka.clients.producer.internals.RecordAccumulator 类 如下代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// RecordAccumulator 类中存放双向队列 Deque&lt;ProducerBatch&gt;</span></span><br><span class="line"><span class="comment">// map 的 key 就是 主题分区，value 就是一组 ProducerBatch</span></span><br><span class="line">ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt; batches;</span><br></pre></td></tr></table></figure>

<p>这对特定大小的 ByteBuffer 缓存进 BufferPool中，大小的定义由 <code>batch.size</code> 参数，默认 16384B，即 16KB</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 由于消息都是以 byte 字节形式传输，RecordAccumulator 提供了 BufferPool 实现对 ByteBuffer 的复用，以实现缓存的高效利用</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> BufferPool free;</span><br></pre></td></tr></table></figure>

<p><strong>Sender</strong>：发送消息线程，从 RecordAccmulator 获取 <code>&lt;Position,Deque&lt;ProducerBatch&gt;&gt;</code> 并转换成 <code>&lt;nodeId, List&lt;ProducerBath&gt;&gt;</code> nodeId 表示 Kafka 集群的 Broker 节点 id</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//org.apache.kafka.clients.producer.internals.Sender</span></span><br><span class="line">Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; batches = <span class="keyword">this</span>.accumulator.drain(cluster, result.readyNodes,</span><br></pre></td></tr></table></figure>

<p><strong>InFlightRequests</strong> ：保存 Sender 线程发送消息后或正在发送,但没有收到响应的请求 。可以通过<code>max.in.flight.requests.per.connection</code> 默认为 5 ，来定义缓存为响应请求的个数。 </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Map&lt;String, Deque&lt;InFlightRequest&gt;&gt; requests = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br></pre></td></tr></table></figure>

<h3 id="元数据的更新"><a href="#元数据的更新" class="headerlink" title="元数据的更新"></a>元数据的更新</h3><p><strong>元数据</strong>：它可以是分区数量、broker 节点地址、端口号、每个分区 leader、follower 副本分配在那些节点等。</p>
<p><strong>leastLoadedNode</strong>：所有 Node 中负载最小的那一个，优先选择它来发送请求。比如 元数据请求、消费者组播协议的交互。</p>
<p>通过 <code>org.apache.kafka.clients.NetworkClient#leastLoadedNode</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Node <span class="title">leastLoadedNode</span><span class="params">(<span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 调用 InFlightRequests 的 count(string nodeId) 计算每个 nodeId 在对缓存的请求队列 Size</span></span><br><span class="line"> 	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nodes.size(); i++) &#123;</span><br><span class="line">  		<span class="keyword">int</span> currInflight = 													<span class="keyword">this</span>.inFlightRequests.count(node.idString());</span><br><span class="line">	 &#125;</span><br><span class="line"><span class="comment">// 之后一系列的判断 找出 size 最小的 node 节点 ，也就是最小负载的node</span></span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果没有指定主题信息，或者超过 <code>metadata.max.age.ms</code> 时间没有更新元数据都会引起元数据的更新操作。默认为 <code>300000</code>，即 <code>5</code> 分钟</p>
<p>当需要更新的时候会挑选出 <strong>leastLoadedNode</strong> ，向这 node 发送 MetadataRequest 请求来获取元数据信息。这个操作是由 Sender 线程负责，当主线程需要这些信息的时候，数据同步使用 <code>synchronized</code> 和 <code>final</code> 关键字来保障</p>
<h2 id="重要的生产者参数"><a href="#重要的生产者参数" class="headerlink" title="重要的生产者参数"></a>重要的生产者参数</h2><h3 id="1-acks"><a href="#1-acks" class="headerlink" title="1 acks"></a>1 acks</h3><p>指定分区中必须要有多少副本收到这条消息,生产者才认为消息成功写入</p>
<ul>
<li>acks = ‘1’。默认值 ‘1’ 。生产者发送消息后，只要消息无法写入 leader 副本，那么生产者就会收到一个错误的响应</li>
<li>acks = ‘0’。生产者发送消息之后不需要等待任何服务端的响应</li>
<li>acks = ‘-1’ 或 ‘all’ 。生产者发送消息后，需等待 ISR 中的所有副本都成功写入消息之后才能收到来自服务端的成功响应</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">properties.put(ProducerConfig.ACKS_CONFIG, <span class="string">"all"</span>);</span><br></pre></td></tr></table></figure>

<h3 id="2-max-request-size"><a href="#2-max-request-size" class="headerlink" title="2 max.request.size"></a>2 max.request.size</h3><p>用来限制生产者客户端能发送的消息的最大值，默认值为 <code>1048576B</code> ，即 <code>1MB</code>。这个参数配置需要注意 broker 端的 <code>message.max.bytes</code> 参数配置的连锁反应。</p>
<h3 id="3-retires-和-retry-backoff-ms"><a href="#3-retires-和-retry-backoff-ms" class="headerlink" title="3 retires 和 retry.backoff.ms"></a>3 retires 和 retry.backoff.ms</h3><p><code>retires</code> 生产者重试次数，默认值为 0 </p>
<p><code>retry.backoff.ms</code> 两次重试之间的间隔，默认 100</p>
<h3 id="4-compression-type"><a href="#4-compression-type" class="headerlink" title="4 compression.type"></a>4 compression.type</h3><p>自定消息的压缩方式，默认为 <code>&quot;none&quot;</code> ，默认不压缩，可选配置：<code>gizp</code> <code>snappy</code> 和 <code>lz4</code></p>
<h3 id="5-connections-max-idle-ms"><a href="#5-connections-max-idle-ms" class="headerlink" title="5 connections.max.idle.ms"></a>5 connections.max.idle.ms</h3><p>指定多久之后关闭限制的连接，默认是 544000，即 9 分钟</p>
<h3 id="6-linger-ms"><a href="#6-linger-ms" class="headerlink" title="6 linger.ms"></a>6 linger.ms</h3><p>指生产者发送 ProducerBatch 之前等待更多消息 ProduerRecord 加入 ProduerBatch 的时间，默认为 0 。生产者会在 ProduerBatch 被填满或者等待时间超过配置的值时发送出去</p>
<h3 id="7-receive-buffer-bytes"><a href="#7-receive-buffer-bytes" class="headerlink" title="7 receive.buffer.bytes"></a>7 receive.buffer.bytes</h3><p>设置 Socket 接收消息缓冲区 SO_RECBUF 的大小，默认值为 32768B ，即 128KB。如果设置为 -1 ，则使用操作系统的默认值。如果 Produer 与 kafka 处于不同的机房，则可以适当的调大这个参数值</p>
<h3 id="8-send-buffer-bytes"><a href="#8-send-buffer-bytes" class="headerlink" title="8 send.buffer.bytes"></a>8 send.buffer.bytes</h3><p>设置 Socket 发送消息缓冲区 SO_SNDBUF 的大小，默认值为 131072B，即 128KB。如果设置为 -1 ，则使用操作系统的默认值。</p>
<h3 id="9-request-timeout-ms"><a href="#9-request-timeout-ms" class="headerlink" title="9 request.timeout.ms"></a>9 request.timeout.ms</h3><p>设置 Producer 等待请求响应的最长时间，默认值为 30000ms 。请求超时之后可以选择进行重试。此配置需要比 broker 端参数 <code>replica.lag.time.max.ms</code> 的值要大，这样可以减少因客户端重试而引起的消息重复概率</p>
<h1 id="3-消费者"><a href="#3-消费者" class="headerlink" title="[3] 消费者"></a>[3] 消费者</h1><h2 id="消费者与消费组"><a href="#消费者与消费组" class="headerlink" title="消费者与消费组"></a>消费者与消费组</h2><p><strong>消费者</strong> ：在同一个消费组下的消费者只会有一个消费到消息，不在同一个消费组则每个消费组内会有一个消费者消费到消息。</p>
<p><strong>消费组</strong>：kafka 特有的概念，每个消费者都必须指定一个消费组。</p>
<ul>
<li>如果所有的消费者都属于同一个消费组，那么所有的消息都会被均衡的投递给每一个消费者，即每条消息只会被一个消费者处理，这就相当于点对点模式</li>
<li>如果所有的消费者都属于不同的消费组，那么所有的消息都会被广播给所有的消费者，即每条消息都会被所有的消费者处理，这就相当于发布/订阅模式</li>
</ul>
<h2 id="消费者客户端"><a href="#消费者客户端" class="headerlink" title="消费者客户端"></a>消费者客户端</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumerAnalysis</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> String brokerList = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"topic-demo"</span>;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> String groupId = <span class="string">"group.demo1"</span>;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> AtomicBoolean isRunning = <span class="keyword">new</span> AtomicBoolean(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Properties <span class="title">initConfig</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, 							StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, 							StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);</span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);</span><br><span class="line">        properties.put(ConsumerConfig.CLIENT_ID_CONFIG, <span class="string">"consumer.client.id.dmeo"</span>);</span><br><span class="line">        <span class="keyword">return</span> properties;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties properties = initConfig();</span><br><span class="line">        KafkaConsumer&lt;String,String&gt; consumer = <span class="keyword">new</span> KafkaConsumer(properties);</span><br><span class="line">        consumer.subscribe(Arrays.asList(topic));</span><br><span class="line">        <span class="keyword">while</span> (isRunning.get()) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records =</span><br><span class="line">                consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">            records.forEach(e -&gt; &#123;</span><br><span class="line">                System.out.println(e.toString());</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="订阅主题与分区"><a href="#订阅主题与分区" class="headerlink" title="订阅主题与分区"></a>订阅主题与分区</h2><p>主要分析 <code>KafkaConsumer</code> 类 中的方法</p>
<p><strong>subscribe</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 一种是集合类入参说明可以定义多个主题,一种是正则表达式</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener)</span> </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Collection&lt;String&gt; topics)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Pattern pattern, ConsumerRebalanceListener listener)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Pattern pattern)</span></span></span><br></pre></td></tr></table></figure>

<p><strong>assign</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 指定分区 注意这里是 TopicPartition</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">assign</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span></span><br><span class="line"><span class="function"><span class="comment">// 例如: </span></span></span><br><span class="line"><span class="function">consumer.<span class="title">assign</span><span class="params">(Arrays.asList(new TopicPartition(<span class="string">"topic-demo"</span>, <span class="number">0</span>)</span>))</span>;</span><br></pre></td></tr></table></figure>

<p>如果不知道主题下有多少分区 可使用 <code>partitionFor</code>  查看该主题下有多少分区</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> List&lt;PartitionInfo&gt; <span class="title">partitionsFor</span><span class="params">(String topic)</span></span></span><br></pre></td></tr></table></figure>

<p><strong>unSubscibe</strong></p>
<p>取消订阅主题</p>
<h2 id="反序列化"><a href="#反序列化" class="headerlink" title="反序列化"></a>反序列化</h2><p>和 生产者 的序列化使用模式一致，继承 <code>Deserializer</code></p>
<h2 id="消息消费"><a href="#消息消费" class="headerlink" title="消息消费"></a>消息消费</h2><p>消息的消费一般有两种模式：推 和 拉 模式。推模式是服务端主动将消息推给消费者，而拉模式是消费者主动向服务端发起请求来拉取消息。 kafka 采用的是拉模式</p>
<p>调用 <code>poll()</code> 方法拉取消息</p>
<h2 id="消息位移"><a href="#消息位移" class="headerlink" title="消息位移"></a>消息位移</h2><h3 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h3><p>kafka 分区中的每条消息都有唯一 offset ，用于表示消息在分区中对应的位置。利用这个 offset 就可以知道消息在分区的哪个位置</p>
<p>每次调用 <code>poll()</code>方法，它返回的是还没被消费过消息集，要做到这一点就需要记录上一次的消费位移 offset</p>
<h3 id="自动位移提交"><a href="#自动位移提交" class="headerlink" title="自动位移提交"></a>自动位移提交</h3><p>在 kafka 中默认自动提交消费位移，由客户端参数<code>enable.auto.commit</code> 配置值为 <code>true</code> 。在 <code>poll()</code> 方法内部，实现了自动位移提交动作的逻辑</p>
<p>自动提交消费位移的方式非常方便，我们不需要写额外的同步位移代码。在默认的自动提交方式 消费者是<strong>每隔 5 秒</strong> 拉取每个分区最大的消息位移进行提交。假设刚刚提交完一次消费位移，然后拉取一批消息进行消费时，消费者奔溃了，恢复后又会从上一次位移提交的地方重新开始消费，这就照成了 <strong>重复消费</strong> 的现象</p>
<h3 id="手动位移提交"><a href="#手动位移提交" class="headerlink" title="手动位移提交"></a>手动位移提交</h3><p><code>enable.auto.commit</code> 配置为 <code>false</code></p>
<p>手动提交细分为两种同步提交和异步提交，即 commitSync 和 commitAsync</p>
<h4 id="commitSync-同步提交"><a href="#commitSync-同步提交" class="headerlink" title="commitSync 同步提交"></a>commitSync 同步提交</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> (isRunning.get()) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">    <span class="keyword">for</span>(ConsumerRecord&lt;String,String&gt; record : records)&#123;</span><br><span class="line">        <span class="comment">// do some logical processing</span></span><br><span class="line">    &#125;</span><br><span class="line">    consumer.commitSync();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>更加细粒度同步提交消费位移</strong></p>
<p>使用 <code>commitSync(final Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets)</code> 方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//每条消息都细粒度的同步消费位移，性能极低</span></span><br><span class="line"><span class="keyword">while</span> (isRunning.get()) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">    records.forEach(record -&gt; &#123;</span><br><span class="line">        System.out.println(record.toString());</span><br><span class="line">        <span class="keyword">long</span> offset = record.offset();</span><br><span class="line">        TopicPartition partition = <span class="keyword">new</span> TopicPartition(record.topic(), 																	record.partition());</span><br><span class="line">        consumer.commitSync(Collections.singletonMap(partition, <span class="keyword">new</span> 														OffsetAndMetadata(offset + <span class="number">1</span>)));</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 通常使用 按分区粒度同步消费位移</span></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">    records.partitions().forEach(partition -&gt; &#123;</span><br><span class="line">        List&lt;ConsumerRecord&lt;String, String&gt;&gt; partitionRecords = 	 																 records.records(partition);</span><br><span class="line">        partitionRecords.forEach(record -&gt; &#123;</span><br><span class="line">            <span class="comment">// do some logical proccessing.</span></span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 定位最后一条消息的 offset</span></span><br><span class="line">        <span class="keyword">long</span> lastConsumeOfset = partitionRecords.get(partitionRecords.size() -   														 <span class="number">1</span>).offset();</span><br><span class="line">        consumer.commitSync(Collections.singletonMap(partition, <span class="keyword">new</span> 												OffsetAndMetadata(lastConsumeOfset + <span class="number">1</span>)));</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="commitAsync-异步提交"><a href="#commitAsync-异步提交" class="headerlink" title="commitAsync 异步提交"></a>commitAsync 异步提交</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">consumer.commitAsync((offsets, exception) -&gt; &#123;</span><br><span class="line">    <span class="keyword">if</span> (exception == <span class="keyword">null</span>) &#123;</span><br><span class="line">        System.out.println(offsets);</span><br><span class="line">    &#125; <span class="keyword">else</span></span><br><span class="line">        System.out.println(<span class="string">"fail to commit offsets "</span> + offsets + exception);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>



<h2 id="控制或关闭消费"><a href="#控制或关闭消费" class="headerlink" title="控制或关闭消费"></a>控制或关闭消费</h2><p><code>kafkaConsumer</code> 中使用 <code>pause()</code> 和 <code>resume()</code> 来暂停对某些分区在拉取操作时返回数据给客户端 和 恢复某些分区向客户端返回数据的操作</p>
<h2 id="指定位移消费"><a href="#指定位移消费" class="headerlink" title="指定位移消费"></a>指定位移消费</h2><p>当一个新的消费组建立的时候，它没有可以查找的消费位移，或者消费组内的一个新的消费者订阅了新的主题，它也没有可以查找的消费位移。</p>
<p>在 kafka 中 如果消费者找不到所记录的消费位移时，就会根据消费者客户端参数 <code>auto.offset.reset</code> 的配置来决定从何处开始消费。</p>
<p><code>auto.offset.reset</code></p>
<ul>
<li>latest（默认） 表示从分区末尾开始消费消息</li>
<li>earliest 表示从 0 开始消费</li>
<li>none 不从末尾也不从头消费，而是报错 <code>NoOffsetForPartitionException</code> 异常</li>
</ul>
<h3 id="seek"><a href="#seek" class="headerlink" title="seek()"></a>seek()</h3><p>当想自由的细粒度操作时，使用 <code>KafkaConsumer</code> 中的 <code>seek()</code> 方法，指定从哪个位置开始消费</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seekTest</span><span class="params">()</span></span>&#123;</span><br><span class="line">    KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br><span class="line">    consumer.subscribe(Arrays.asList(topic));</span><br><span class="line">    consumer.poll(Duration.ofMillis(<span class="number">10000</span>));<span class="comment">// 随意设置 10 秒 等待时间</span></span><br><span class="line">    Set&lt;TopicPartition&gt; assignment = consumer.assignment();</span><br><span class="line">    <span class="keyword">for</span> (TopicPartition tp :assignment)&#123;</span><br><span class="line">        consumer.seek(tp, <span class="number">10</span>);<span class="comment">// 将在 offset 10 的位置开始消费</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (isRunning.get())&#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records =    					 											consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">        records.forEach(e -&gt; System.out.println(e.toString()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>优化等待时间 10 秒 照成的浪费</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">    Set&lt;TopicPartition&gt; assignment = <span class="keyword">new</span> HashSet();</span><br><span class="line">    <span class="keyword">while</span> (assignment.size() == <span class="number">0</span>) &#123;</span><br><span class="line">        consumer.poll(Duration.ofMillis(<span class="number">100</span>)); <span class="comment">// 设置0.1 秒 重试 直到拉取完成</span></span><br><span class="line">        assignment = consumer.assignment();</span><br><span class="line">    &#125;</span><br><span class="line">	assignment.forEach(tp -&gt; consumer.seek(tp, <span class="number">10</span>));</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>使用 <code>seek()</code> 从分区末尾消费，<code>endOffSets</code> 获取每个分区最后一个 offset 位置。对应的还有 <code>beginningOffsets()</code>  方法获取初始位置</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="comment">// 从末尾开始消费</span></span><br><span class="line">Map&lt;TopicPartition, Long&gt; offsets = consumer.endOffsets(assignment);</span><br><span class="line">assignment.forEach(tp -&gt; consumer.seek(tp, offsets.get(tp)));</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// kafkaConsumer 直接就提供了 从头 从未消费方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seekToBeginning</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seekToEnd</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span></span><br></pre></td></tr></table></figure>

<p>指定时间消费，如下是指定从 昨天的当前时间开始消费</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">Map&lt;TopicPartition, Long&gt; timestampToSearch = <span class="keyword">new</span> HashMap();</span><br><span class="line">assignment.forEach(tp -&gt; timestampToSearch.put(tp, System.currentTimeMillis() - <span class="number">1</span> * <span class="number">24</span> * <span class="number">3600</span> * <span class="number">1000</span>));<span class="comment">// 从指定的时间开始消费</span></span><br><span class="line">Map&lt;TopicPartition, OffsetAndTimestamp&gt; offsets = consumer.offsetsForTimes(timestampToSearch);</span><br><span class="line">assignment.forEach(tp -&gt; &#123;</span><br><span class="line">    OffsetAndTimestamp offsetAndTimestamp = offsets.get(tp);</span><br><span class="line">    <span class="keyword">if</span> (offsetAndTimestamp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">        consumer.seek(tp, offsetAndTimestamp.offset());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">...</span><br></pre></td></tr></table></figure>



<h2 id="再均衡"><a href="#再均衡" class="headerlink" title="再均衡"></a>再均衡</h2><p>指分区的所属权从一个消费者转移到另一个消费者的行为，它为消费组具备高可用性和伸缩性提供保障。在均衡发生期间，消费组内的消费者不可用。</p>
<p><code>onPartitionsRevoked</code> 该方法会在再均衡开始之前和消费者停止读取消息之后调用</p>
<p><code>onPartitionsAssigned</code> 该方法会在重新分配分区之后和消费者开始读取消息之前被调用</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">consumer.subscribe(Arrays.asList(topic), <span class="keyword">new</span> ConsumerRebalanceListener() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">		<span class="comment">// 可以存储 offset 到 数据库</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">    	<span class="comment">// 可以从数据库中取出 offset  配合 seek()</span></span><br><span class="line">        partitions.foreach(tp -&gt; consumer.seek(tp, getOffsetFromDB(tp)));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>



<h2 id="消费者拦截器"><a href="#消费者拦截器" class="headerlink" title="消费者拦截器"></a>消费者拦截器</h2><p>消费者自定义实现 <code>ConsumerInterceptor</code> 接口</p>
<ul>
<li>onConsumer() KafkaConsumer 会在 poll() 方法返回值之前调用</li>
<li>onCommit() KafkaConsumer 会在提交完消费位移之后调用拦截器</li>
</ul>
<p>拦截器可以实现对消息的验证过滤</p>
<h2 id="重要的消费者参数"><a href="#重要的消费者参数" class="headerlink" title="重要的消费者参数"></a>重要的消费者参数</h2><p>百度吧</p>
<h2 id="使用多线程提高消费能力"><a href="#使用多线程提高消费能力" class="headerlink" title="使用多线程提高消费能力"></a>使用多线程提高消费能力</h2><ul>
<li>使用 <code>SynchronousQueue</code> 一进一出队列，避免队列里缓冲数据，这样在系统异常关闭时，就能排除因为阻塞队列丢失消息的可能</li>
<li>使用 <code>ThreadPoolExecutor.CallerRunsPolicy()</code> 饱和策略，使得多线程处理不过来的时候，能够阻塞在 kafka 的消费线程上</li>
<li>相关参数配置</li>
</ul>
<p><strong>max.poll.records</strong></p>
<p>调用一次poll，返回的最大条数。这个值设置的大，那么处理的就慢，很容易超出<code>max.poll.interval.ms</code>的值（默认5分钟），造成消费者的离线。在耗时非常大的消费中，是需要特别注意的。</p>
<p><strong>enable.auto.commit</strong></p>
<p>是否开启自动提交（offset）如果开启，consumer已经消费的offset信息将会间歇性的提交到kafka中（持久保存）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer(properties);</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">        <span class="keyword">while</span> (isRunning.get()) &#123;</span><br><span class="line">            consumer.subscribe(Collections.singletonList(topic));</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records =   	       														consumer.poll(Duration.ofMillis(<span class="number">100</span>));</span><br><span class="line">            records.forEach(record -&gt; &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    queue.put(record);</span><br><span class="line">                    consumer.commitSync();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;).start();</span><br><span class="line">    initPipelineThread();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">processRecordItem</span><span class="params">(ConsumerRecord&lt;String, String&gt; record)</span> </span>&#123;</span><br><span class="line">    System.out.println(Thread.currentThread().getName() + record.toString());</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">initPipelineThread</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (isRunning.get()) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">final</span> ConsumerRecord&lt;String, String&gt; record = queue.poll(<span class="number">5</span>,  															TimeUnit.MINUTES);</span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">null</span> != record)</span><br><span class="line">                executor.submit(() -&gt; &#123;</span><br><span class="line">                    processRecordItem(record);</span><br><span class="line">                &#125;);	</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里是测试用例，<strong>实际项目中用法可参考文章</strong>：<br><a href="https://segmentfault.com/a/1190000018640106" target="_blank" rel="noopener">使用多线程增加kafka消费能力</a></p>
<h1 id="4-主题与分区"><a href="#4-主题与分区" class="headerlink" title="[4] 主题与分区"></a>[4] 主题与分区</h1><p>​    主题座位消息的归类，可以在媳妇为一个或多个分区，分区也可以看作对消息的二次归类。分区的划分不仅为 kafka 提供了可伸缩性，水平扩展的功能，还通过多副本机制来为 kafka 提供数据冗余以提高数据可靠性</p>
<h2 id="创建主题"><a href="#创建主题" class="headerlink" title="创建主题"></a>创建主题</h2><p>broker端配置参数:</p>
<p><strong>auto.create.topic.enalbe</strong>  默认为 true</p>
<p>当生产者向一个尚未创建的主题发送消息时，会自动创建一个分区数为 <strong>num.partitions</strong>(默认值1) 、副本因子为 <strong>default.replication.factor</strong>(默认值1) 的主题</p>
<p>当消费者向一个未知主题读取消息时，或者当任意一个客户端向未知主题发送元数据请求时，都会按照配置参数 <strong>num.partitions</strong>、<strong>default.replication.factor</strong> 的值来创建一个相应的主题</p>
<p>使用 kafka-topics,.sh 脚本来创建主题实际上是调用了 kafka.admin.TopicCommand 类</p>
<p>创建一个分区数为 4 、副本因子为 2 的主题 demo</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">bin\kafka-topics.sh --zookeeper localhost:2181/kafka --create --topic demo --partitions 4 --replication-factor 2</span><br></pre></td></tr></table></figure>

<p>执行完脚本后，kafka 会在 <code>log.dir</code> 或 <code>log/dirs</code> 参数所配置的目录下创建相应的主题分区，默认目录为<code>/tmp/kafka-logs</code>， 这取决于你的 <code>server.properties</code> 配置</p>
<p>根据 kafka 的日志文件查看该节点下创建的主题分区</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">ls -al /opt/kafka/logs/ | grep topic-create</span><br><span class="line"></span><br><span class="line">drwxr-xr-x.  2 root root  141 12月 28 19:25 --topic-create-1</span><br><span class="line">drwxr-xr-x.  2 root root  141 12月 28 19:25 --topic-create-2</span><br><span class="line">drwxr-xr-x.  2 root root  141 12月 28 19:25 --topic-create-3</span><br></pre></td></tr></table></figure>





<p>根据 zookeeper 客户端获取——当创建一个主题会在 zookeeper 的 <code>/brokers/topics</code> 目录下创建同名的实节点</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">get /brokers/topics/demo</span><br><span class="line">&#123;<span class="string">"version"</span>:1,<span class="string">"partitions"</span>:&#123;<span class="string">"2"</span>:[1,2],<span class="string">"1"</span>:[0,1],<span class="string">"3"</span>:[2,1],<span class="string">"0"</span>:[2,0]&#125;&#125;</span><br></pre></td></tr></table></figure>

<p>返回的数据 —— “2” : [1,2] 表示分区 2 分配了 2 个副本，分别在 brokerId 为 1 和 2 的 broker 节点中</p>
<p><img src="../images/1577082082(1).jpg" alt="关系图"></p>
<h3 id="查看主题"><a href="#查看主题" class="headerlink" title="查看主题"></a>查看主题</h3><p>​    通过 list 指令可以查看当前所有可用的主题</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">bin\kafka-topics.sh --zookeeper localhost:2181/kafka -list</span><br></pre></td></tr></table></figure>

<p>​    通过 describe 指令查看所有主题详细信息。–topic 指定主题分析</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">bin\kafka-topics.sh --zookeeper localhost:<span class="number">2181</span>/kafka --descibe --topic demo</span><br></pre></td></tr></table></figure>



<h3 id="修改主题"><a href="#修改主题" class="headerlink" title="修改主题"></a>修改主题</h3><p>当主题被创建之后，可以修改分区个数、修改配置等，由 kafka-topics.sh 脚本中的 alter 指令提供</p>
<p>增加主题demo的分区数</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181/kafka --alter --topic demo --partitions 3</span><br></pre></td></tr></table></figure>

<p>修改分区数会影响既定的消息顺序，当主题中的消息包含 key 时，根据 key 计算分区的行为就会收到影响</p>
<p>建议在一开始就设置好分区数量，避免以后对其进行调整。</p>
<p>目前 kafka 只支持增加分区数而不支持减少分区数</p>
<p>修改主题配置<code>max.messgae.bytes=20000</code></p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181/kafka --alter --topic demo --config max.messgae.bytes=20000</span><br></pre></td></tr></table></figure>



<h3 id="删除主题"><a href="#删除主题" class="headerlink" title="删除主题"></a>删除主题</h3><p>使用 kafka-topics.sh 脚本中的 delete 指令删除主题demo</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181/kafka --delete --topic demo</span><br></pre></td></tr></table></figure>

<p>手动方式删除主题</p>
<p>主题中的元数据存储在 zookeeper 中的 /broker/topics 和 /config/topics 路径下</p>
<p>主题中的消息存储在 log.dir 或 log.dirs 配置的路径下</p>
<ul>
<li>1 删除 zookeeper 中的节点  /config/topics/demo</li>
<li>rmr /config/topics/demo</li>
<li>2 删除 zookeeper 中的节点 /brokers/topics/demo</li>
<li>delete /brokers/topics/demo</li>
<li>3 删除集群中所有与主题 topics-delete 有关的文件</li>
<li>集群中的各个 broker 节点中执行 rm -rf /tmp/kafka-logs/demo</li>
</ul>
<h3 id="使用-kafkaAdminClient"><a href="#使用-kafkaAdminClient" class="headerlink" title="使用 kafkaAdminClient"></a>使用 kafkaAdminClient</h3><p>创建主题</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> String brokerList = <span class="string">"localhost:9092"</span>;</span><br><span class="line"><span class="keyword">static</span> String topic = <span class="string">"topic-admin"</span>;</span><br><span class="line"><span class="keyword">static</span> Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line">    properties.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);</span><br><span class="line">    properties.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, <span class="number">30000</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">create</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    AdminClient client = AdminClient.create(properties);</span><br><span class="line">    NewTopic newTopic = <span class="keyword">new</span> NewTopic(topic, <span class="number">4</span>, (<span class="keyword">short</span>) <span class="number">1</span>);</span><br><span class="line">    CreateTopicsResult result = client.createTopics(Collections.singleton(newTopic));</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        result.all().get();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    client.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>分析主题</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">describeTopic</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    AdminClient client = AdminClient.create(properties);</span><br><span class="line">    DescribeTopicsResult describeTopicsResult = client.describeTopics(Collections.singleton(topic));</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        Map&lt;String, TopicDescription&gt; stringTopicDescriptionMap =</span><br><span class="line">            describeTopicsResult.all().get();</span><br><span class="line">        System.out.println(stringTopicDescriptionMap);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>更多操作请百度</p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2019/12/31/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
</search>
