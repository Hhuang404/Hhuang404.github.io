<!DOCTYPE html>




<html class="theme-next gemini" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="this is my blog">
<meta property="og:type" content="website">
<meta property="og:title" content="huangh">
<meta property="og:url" content="https://hhuang404.github.io/index.html">
<meta property="og:site_name" content="huangh">
<meta property="og:description" content="this is my blog">
<meta property="article:author" content="huanghao">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://hhuang404.github.io/"/>





  <title>huangh</title>
  








<meta name="generator" content="Hexo 4.2.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">huangh</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">HUANGHAO</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://hhuang404.github.io/2020/02/05/Canal-%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%BD%BF%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="huanghao">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="huangh">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/05/Canal-%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%BD%BF%E7%94%A8/" itemprop="url">Canal_配置与使用</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-05T10:36:42+08:00">
                2020-02-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E6%8A%80%E6%9C%AF%E6%A0%88/" itemprop="url" rel="index">
                    <span itemprop="name">分布式架构技术栈</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2020/02/05/Canal-%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%BD%BF%E7%94%A8/" class="leancloud_visitors" data-flag-title="Canal_配置与使用">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h3><p>利用 Canal 监听数据库中数据的变化，解决缓存与数据库中的数据不一致的问题。</p>
<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong>canal [kə’næl]</strong>，译意为水道/管道/沟渠，主要用途是基于 MySQL 数据库增量日志解析，提供增量数据订阅和消费</p>
<p>基于日志增量订阅和消费的业务包括</p>
<ul>
<li>数据库镜像</li>
<li>数据库实时备份</li>
<li>索引构建和实时维护(拆分异构索引、倒排索引等)</li>
<li>业务 cache 刷新</li>
<li>带业务逻辑的增量数据处理</li>
</ul>
<h3 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h3><h4 id="canal-工作原理"><a href="#canal-工作原理" class="headerlink" title="canal 工作原理"></a>canal 工作原理</h4><ul>
<li>canal 模拟 MySQL slave 的交互协议，伪装自己为 MySQL slave ，向 MySQL master 发送dump 协议</li>
<li>MySQL master 收到 dump 请求，开始推送 binary log 给 slave (即 canal )</li>
<li>canal 解析 binary log 对象(原始为 byte 流)</li>
</ul>
<h3 id="安装与配置"><a href="#安装与配置" class="headerlink" title="安装与配置"></a>安装与配置</h3><p>使用 Docker 容器安装</p>
<p><strong>step 1 拉取镜像</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull canal/canal-server:latest</span><br></pre></td></tr></table></figure>

<p><strong>step 2 安装</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker -run -p 11111:11111 --name canal -d docker.io/canal/canal-server</span><br></pre></td></tr></table></figure>

<p><strong>step 3 配置</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">进入 canal ：docker <span class="built_in">exec</span> -it canal /bin/bash </span><br><span class="line">进入配置目录 ：<span class="built_in">cd</span> /canal-server/conf </span><br><span class="line">canal.properites 是 canal 自身的配置 它相当于一个从数据库</span><br><span class="line">进入 example 目录 ： <span class="built_in">cd</span> /canal-server/conf/example</span><br><span class="line">编辑配置文件 ：vi instace.properties</span><br></pre></td></tr></table></figure>

<p><strong>step 4 修改配置信息</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#数据库地址</span></span><br><span class="line">canal.instance.master.address=172.18.0.2:3306</span><br><span class="line"><span class="comment">#数据库账号密码</span></span><br><span class="line">canal.instance.dbUsername=canal</span><br><span class="line">canal.instance.dbPassword=canal</span><br><span class="line"><span class="comment">#使用正则或指定扫描数据库表</span></span><br><span class="line">canal.instance.filter.regex=.*\\..* <span class="comment"># 扫描所有数据库所有表</span></span><br><span class="line">canal.instance.filter.regex=changgou_content.*<span class="comment"># canggou_content 数据库中的所有表</span></span><br><span class="line"><span class="comment">#canal mq 主题 和 自己服务中的mq主题一至</span></span><br><span class="line">canal.mq.topic=example</span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://hhuang404.github.io/2020/01/15/ngxin_%E5%B9%B6%E5%8F%91%E9%85%8D%E7%BD%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="huanghao">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="huangh">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/15/ngxin_%E5%B9%B6%E5%8F%91%E9%85%8D%E7%BD%AE/" itemprop="url">ngxin 并发配置</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-15T21:05:35+08:00">
                2020-01-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ngxin/" itemprop="url" rel="index">
                    <span itemprop="name">ngxin</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2020/01/15/ngxin_%E5%B9%B6%E5%8F%91%E9%85%8D%E7%BD%AE/" class="leancloud_visitors" data-flag-title="ngxin 并发配置">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="控制速率"><a href="#控制速率" class="headerlink" title="控制速率"></a>控制速率</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">http&#123;</span><br><span class="line">	# 设置限流配置</span><br><span class="line">    limit_req_zone $binary_remote_addr zone&#x3D;myRateLimit:10m rate&#x3D;2r&#x2F;s;</span><br><span class="line">	...</span><br><span class="line">	</span><br><span class="line">	server &#123;</span><br><span class="line">		location &#x2F; &#123;</span><br><span class="line">			limit_req zone&#x3D;myRateLimit;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>binary_remote_addr 是一种key，表示基于 remote_addr(客户端IP) 来做限流，binary_ 的目的是压缩内存占用量。<br>zone：定义共享内存区来存储访问信息， myRateLimit:10m 表示一个大小为10M，名字为myRateLimit的内存区域。1M能存储16000 IP地址的访问信息，10M可以存储16W IP地址访问信息。<br>rate 用于设置最大访问速率，rate=10r/s 表示每秒最多处理10个请求。Nginx 实际上以毫秒为粒度来跟踪请求信息，因此 10r/s 实际上是限制：每100毫秒处理一个请求。这意味着，自上一个请求处理完后，若后续100毫秒内又有请求到达，将拒绝处理该请求.</p>
<h2 id="处理突发流量"><a href="#处理突发流量" class="headerlink" title="处理突发流量"></a>处理突发流量</h2><p>上面例子限制 10r/s，如果有时正常流量突然增大，超出的请求将被拒绝，无法处理突发流量，可以结合 <strong>burst</strong> 参数使用来解决该问题。</p>
<p>如下配置表示：</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="attribute">location</span> / &#123;</span><br><span class="line">		<span class="attribute">limit_req</span> zone=myRateLimit burst=<span class="number">20</span>;</span><br><span class="line">        <span class="attribute">root</span>   html;</span><br><span class="line">        <span class="attribute">index</span>  index.html index.htm;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>burst 译为突发、爆发，表示在超过设定的处理速率后能额外处理的请求数,当 rate=10r/s 时，将1s拆成10份，即每100ms可处理1个请求。</p>
<p>此处，*<em>burst=20 *</em>，若同时有21个请求到达，Nginx 会处理第一个请求，剩余20个请求将放入队列，然后每隔100ms从队列中获取一个请求进行处理。若请求数大于21，将拒绝处理多余的请求，直接返回503.</p>
<p>不过，单独使用 burst 参数并不实用。假设 burst=50 ，rate依然为10r/s，排队中的50个请求虽然每100ms会处理一个，但第50个请求却需要等待 50 * 100ms即 5s，这么长的处理时间自然难以接受。</p>
<p>因此，burst 往往结合 nodelay 一起使用。</p>
<p>例如：如下配置：</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="attribute">location</span> / &#123;</span><br><span class="line">		<span class="attribute">limit_req</span> zone=myRateLimit burst=<span class="number">5</span> nodelay;</span><br><span class="line">        <span class="attribute">root</span>   html;</span><br><span class="line">        <span class="attribute">index</span>  index.html index.htm;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="控制并发量-连接数"><a href="#控制并发量-连接数" class="headerlink" title="控制并发量 (连接数)"></a>控制并发量 (连接数)</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">limit_conn_zone $binary_remote_addr zone&#x3D;addr:10m : 表示限制根据用户的 ip 地址来显示，设置存储地址为内存大小 10M</span><br><span class="line"></span><br><span class="line">limit_conn addr 2; 表示 同一个地址只允许连接 2 次。</span><br></pre></td></tr></table></figure>
<p>限制每个客户端IP与服务器的连接数，同时限制与虚拟服务器的连接总数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">limit_conn_zone $binary_remote_addr zone&#x3D;perip:10m;</span><br><span class="line">limit_conn_zone $server_name zone&#x3D;perserver:10m;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">location &#x2F;&#123;</span><br><span class="line">	limit_conn perip 10;# 单个客户端ip与服务端的连接数</span><br><span class="line">	limit_conn perserver 100; #限制与服务器的总连接数 谨慎配置</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://hhuang404.github.io/2020/01/01/kafka-%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="huanghao">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="huangh">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/01/kafka-%E5%AD%A6%E4%B9%A0/" itemprop="url">kafka 学习</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-01T10:09:24+08:00">
                2020-01-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2020/01/01/kafka-%E5%AD%A6%E4%B9%A0/" class="leancloud_visitors" data-flag-title="kafka 学习">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="1-Kafka-基础概念"><a href="#1-Kafka-基础概念" class="headerlink" title="[1] Kafka 基础概念"></a>[1] Kafka 基础概念</h1><p>​    Kafka 起初是由 LinkedIn 公司采用 Scala 语言开发的一个多分区, 多副本且基于 ZooKeeper 协调的分布式消息系统.</p>
<p>Kafka 支持三大功能 :</p>
<ul>
<li><strong>消息系统</strong>：Kafka 和传统消息一样 都具备 系统解耦、冗余存储、 流量消锋、缓冲、异步通信、扩展性、可恢复性等功能。除此之外 Kafka 还支持同分区下消息的顺序性以及回溯消费功能</li>
<li><strong>存储系统</strong>：Kafka 把消息持久化到磁盘，相比其他基于内存存储的系统而言，有效的减低了数据丢失的风险。</li>
<li><strong>流式处理平台</strong>：Kafka 提供了一个完整的流式处理类库</li>
</ul>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>​    Kafka 体系架构包括若干个 Producer、Broker、Conusmer，以及一个 Zookeeper 集群。</p>
<p>​    Producer 将消息发送到 Broker，Broker 负责将受到的消息存储到磁盘中，而 Consumer 负责从 Broker 订阅并消费消息</p>
<p><strong>Zookeeper</strong>：是 Kafka 用来负责集群元数据的管理、控制器的选举等操作。</p>
<p><strong>Producer</strong>：是 生产者，发送消息的一方</p>
<p><strong>Consumer</strong>： 是消费者，接受消息的一方</p>
<p><strong>Broker</strong>：服务代理节点。对于 Kafka 而言，Broker 可以简单的看作一个独立的 Kafka 服务节点或 Kafka 服务实例，一个或多个 Broker 组成一个 Kafka 集群。</p>
<p><strong>Topic</strong>：发送消息前需要先定义一个主题</p>
<p><strong>Position</strong>：分区，在创建 Topic 时可自定分区数量，分区存放消息，利用消息的 Offset 保证 分区内消息的顺序性。也可以在创建完 Topic 后再修改分区数量，实现水平扩展</p>
<p><strong>Offset</strong>：消息在被追加到分区日志文件时会分配一个偏移量 offset</p>
<p><strong>Replica</strong>：为分区引入多副本机制，可以通过增加副本的数量来提升容灾能力。同一分区中不同的副本保存的消息是一致的，副本之间是一主多从关系，leader 副本负责读写请求，follower 副本只负责与 leader 副本消息同步。副本保存在不同的 broker中， 这样可以避免某个 broker 挂掉后仍然能保证服务可用。</p>
<p><strong>AR</strong>：分区中的所有副本统称 Assigend Replicas </p>
<p><strong>ISR</strong>：所有与 leader 副本保持同步的副本（包括 leader ）组成 In-Sync Replicas</p>
<p><strong>OSR</strong>：所有与 leader 副本同步滞后过多的副本（不包括 leader）组成 Out-of-Sync Replicas</p>
<p>默认情况下只有 ISR 集合中的副本才有资格被选举为新的 leader</p>
<p><strong>HW</strong>：High Watermark 它标识一个特定的消息偏移量 offset，消费者只能拉取到这个 offset 之前的消息。</p>
<p><strong>LEO</strong>： LogEndOffset 当前将要插入的最新消息的 offset </p>
<h1 id="2-生产者"><a href="#2-生产者" class="headerlink" title="[2] 生产者"></a>[2] 生产者</h1><h2 id="生产者客户端"><a href="#生产者客户端" class="headerlink" title="生产者客户端"></a>生产者客户端</h2><h3 id="引入依赖"><a href="#引入依赖" class="headerlink" title="引入依赖"></a>引入依赖</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.0.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<h3 id="配置生产者参数与构建生产者"><a href="#配置生产者参数与构建生产者" class="headerlink" title="配置生产者参数与构建生产者"></a>配置生产者参数与构建生产者</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> KafkaProducer&lt;String,String&gt; <span class="title">initConfig</span><span class="params">()</span></span>&#123;</span><br><span class="line">    Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">    properties.put(<span class="string">"key.serializer"</span>,</span><br><span class="line">                   <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">    properties.put(<span class="string">"value.serializer"</span>,</span><br><span class="line">                   <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">    properties.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">	KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer(properties);</span><br><span class="line">    <span class="keyword">return</span> producer;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="构建待发送消息、发送消息、关闭生产者"><a href="#构建待发送消息、发送消息、关闭生产者" class="headerlink" title="构建待发送消息、发送消息、关闭生产者"></a>构建待发送消息、发送消息、关闭生产者</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    KafkaProducer&lt;String, String&gt; producer = initConfig();</span><br><span class="line">    ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"topic-demo"</span>, <span class="string">"hello!"</span>);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        producer.send(record);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    producer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="必填的参数配置"><a href="#必填的参数配置" class="headerlink" title="必填的参数配置"></a>必填的参数配置</h2><p><strong>bootstrap.servers</strong>：用来自定生产者连接 kafka 集群所需要的 broker 地址，多个则用逗号分割。如果有多个不需要全部列举，但至少填两个以上的 broker ，因为生产者会根据一个 broker 里查找其他的 broker 信息。</p>
<p><strong>key.serializer</strong> 和 <strong>value.serializer</strong>： broker 端接受的消息必须以字节数组（byte[]）的形式存在。这两个参数指定消息的 key 和 value 序列化类型，字符串内容必须是全类名</p>
<p>使用 <strong>ProducerConfig</strong> 代替字符串类型的配置</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"localhost:9092"</span>);</span><br></pre></td></tr></table></figure>



<h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><h3 id="send"><a href="#send" class="headerlink" title="send()"></a>send()</h3><h4 id="采用异步发送消息"><a href="#采用异步发送消息" class="headerlink" title="采用异步发送消息"></a>采用异步发送消息</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="comment">// send 方法是异步的</span></span><br><span class="line">producer.send(record);</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h4 id="采用同步发送消息"><a href="#采用同步发送消息" class="headerlink" title="采用同步发送消息"></a>采用同步发送消息</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="comment">// send 方法是异步的, 可以链式调用 get() 方法导致同步,返回 metadata 对象</span></span><br><span class="line"><span class="comment">// metadata 保存的是消息的元数据信息:当前消息的主题,分区号,偏移量 offset,时间戳等</span></span><br><span class="line"><span class="comment">// get(long timeout, TimeUnit unit) 实现可超时的阻塞</span></span><br><span class="line">RecodeMetadata metadata = producer.send(record).get();</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h4 id="采用异步回调发送消息"><a href="#采用异步回调发送消息" class="headerlink" title="采用异步回调发送消息"></a>采用异步回调发送消息</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">producer.send(record, (metadata, exception) -&gt; &#123;</span><br><span class="line">    <span class="comment">// 发送异常则不为null</span></span><br><span class="line">    <span class="keyword">if</span> (exception != <span class="keyword">null</span>) </span><br><span class="line">        exception.printStackTrace();</span><br><span class="line">    <span class="keyword">else</span> </span><br><span class="line">        System.out.println(metadata.topic() + <span class="string">"-"</span> +</span><br><span class="line">                           metadata.partition() + <span class="string">":"</span> + metadata.offset());</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<h3 id="close"><a href="#close" class="headerlink" title="close()"></a>close()</h3><p>close() 方法会阻塞等待之前所有的发动请求完成后再关闭 kafkaProducer</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// producer.close(long timeout, TimeUnit unit); 可实现带超时时间的</span></span><br><span class="line">producer.close();</span><br></pre></td></tr></table></figure>



<h2 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h2><p>客户端的自带的序列化有：String、ByteBuffer、Bytes、Double、Integer、Long </p>
<p>它们都实现了 <code>org.apache.kafka.common.serialization.Serializer</code> 接口</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 序列化的类都必须实现该接口, 自定义序列化就是实现 Serializer 接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Serializer</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">Closeable</span> </span>&#123;</span><br><span class="line">	<span class="comment">// 用来配置当前类</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs, <span class="keyword">boolean</span> isKey)</span></span>;</span><br><span class="line">	<span class="comment">// 执行序列化操作: 将 data 序列化为 byte[]</span></span><br><span class="line">    <span class="keyword">byte</span>[] serialize(String topic, T data);</span><br><span class="line">	<span class="comment">// 空方法 如果实现了次方法,则必须确保次方法的幂等性,KafakProducer 可能会多次调用</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="分区器"><a href="#分区器" class="headerlink" title="分区器"></a>分区器</h2><p>如果 ProducerRecord 不指定 position 分区 则会使用分区器</p>
<p>默认的分区器 <code>org.apache.kafka.clients.producer.internals.DefaultPartitioner</code></p>
<p>所有分区器都实现 <code>org.apache.kafka.clients.producer.Partitioner</code> 接口</p>
<p><strong>Partitioner</strong> 接口继承 <code>Configurable</code> 类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Partitioner</span> <span class="keyword">extends</span> <span class="title">Configurable</span>, <span class="title">Closeable</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 分区器的具体逻辑</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Configurable</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 可以获取配置信息</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>DefaultPartitioner</strong> 规则如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果 指定的 partition 为空, 但是 key 不为空, 则对 key 进行哈希(采用 murmur2 算法) 得到分区号</span><br><span class="line">如果 指定的 partition 为空, key 为空, 消息会轮询的方式发往主题内的各个分区</span><br></pre></td></tr></table></figure>

<h2 id="生产者拦截器"><a href="#生产者拦截器" class="headerlink" title="生产者拦截器"></a>生产者拦截器</h2><p>自定义的拦截器需要实现 <code>org.apache.kafka.clients.producer.ProducerInterceptor</code> 接口</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ProducerInterceptor</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Configurable</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 发送之前</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ProducerRecord&lt;K, V&gt; <span class="title">onSend</span><span class="params">(ProducerRecord&lt;K, V&gt; record)</span></span>;</span><br><span class="line">    <span class="comment">// 发送之后</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onAcknowledgement</span><span class="params">(RecordMetadata metadata, Exception exception)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>拦截器没有默认的, 自定义拦截器写完后需要在 <code>properties</code> 配置</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">properties.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,</span><br><span class="line">               ProducerInterceptorPrefix<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br></pre></td></tr></table></figure>



<h2 id="2-7-原理分析"><a href="#2-7-原理分析" class="headerlink" title="2.7 原理分析"></a>2.7 原理分析</h2><h3 id="生产者客户端的整体架构："><a href="#生产者客户端的整体架构：" class="headerlink" title="生产者客户端的整体架构："></a>生产者客户端的整体架构：</h3><p><img src="../images/producer.PNG" alt="生产者客户端的整体架构"></p>
<p><strong>RecordAccumulator</strong>：创建后的消息最终进入消息累加器缓存，一定的时间过后 Sender 线程批量发送，进而减少网络传输的资源消耗。缓存的大小通过 <code>buffer.memory</code> 配置，默认 33554432B 即 32MB。如果发送到累加器的速度比发送到服务器的速度快，则 Send() 要么被阻塞，要么爬出异常，取决于参数 <code>max.block.ms</code> 的配置，默认 60000 ，即 60秒</p>
<p><strong>ProducerBatch</strong>：存放一个或多个 ProducerRecord 。</p>
<p>org.apache.kafka.clients.producer.internals.RecordAccumulator 类 如下代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// RecordAccumulator 类中存放双向队列 Deque&lt;ProducerBatch&gt;</span></span><br><span class="line"><span class="comment">// map 的 key 就是 主题分区，value 就是一组 ProducerBatch</span></span><br><span class="line">ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt; batches;</span><br></pre></td></tr></table></figure>

<p>这对特定大小的 ByteBuffer 缓存进 BufferPool中，大小的定义由 <code>batch.size</code> 参数，默认 16384B，即 16KB</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 由于消息都是以 byte 字节形式传输，RecordAccumulator 提供了 BufferPool 实现对 ByteBuffer 的复用，以实现缓存的高效利用</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> BufferPool free;</span><br></pre></td></tr></table></figure>

<p><strong>Sender</strong>：发送消息线程，从 RecordAccmulator 获取 <code>&lt;Position,Deque&lt;ProducerBatch&gt;&gt;</code> 并转换成 <code>&lt;nodeId, List&lt;ProducerBath&gt;&gt;</code> nodeId 表示 Kafka 集群的 Broker 节点 id</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.kafka.clients.producer.internals.Sender</span></span><br><span class="line">Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; batches = <span class="keyword">this</span>.accumulator.drain(cluster, result.readyNodes,</span><br></pre></td></tr></table></figure>

<p><strong>InFlightRequests</strong> ：保存 Sender 线程发送消息后或正在发送,但没有收到响应的请求 。可以通过<code>max.in.flight.requests.per.connection</code> 默认为 5 ，来定义缓存为响应请求的个数。 </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Map&lt;String, Deque&lt;InFlightRequest&gt;&gt; requests = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br></pre></td></tr></table></figure>

<h3 id="元数据的更新"><a href="#元数据的更新" class="headerlink" title="元数据的更新"></a>元数据的更新</h3><p><strong>元数据</strong>：它可以是分区数量、broker 节点地址、端口号、每个分区 leader、follower 副本分配在那些节点等。</p>
<p><strong>leastLoadedNode</strong>：所有 Node 中负载最小的那一个，优先选择它来发送请求。比如 元数据请求、消费者组播协议的交互。</p>
<p>通过 <code>org.apache.kafka.clients.NetworkClient#leastLoadedNode</code></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Node <span class="title">leastLoadedNode</span><span class="params">(<span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 调用 InFlightRequests 的 count(string nodeId) 计算每个 nodeId 在对缓存的请求队列 Size</span></span><br><span class="line"> 	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nodes.size(); i++) &#123;</span><br><span class="line">  		<span class="keyword">int</span> currInflight = 													<span class="keyword">this</span>.inFlightRequests.count(node.idString());</span><br><span class="line">	 &#125;</span><br><span class="line"><span class="comment">// 之后一系列的判断 找出 size 最小的 node 节点 ，也就是最小负载的node</span></span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果没有指定主题信息，或者超过 <code>metadata.max.age.ms</code> 时间没有更新元数据都会引起元数据的更新操作。默认为 <code>300000</code>，即 <code>5</code> 分钟</p>
<p>当需要更新的时候会挑选出 <strong>leastLoadedNode</strong> ，向这 node 发送 MetadataRequest 请求来获取元数据信息。这个操作是由 Sender 线程负责，当主线程需要这些信息的时候，数据同步使用 <code>synchronized</code> 和 <code>final</code> 关键字来保障</p>
<h2 id="重要的生产者参数"><a href="#重要的生产者参数" class="headerlink" title="重要的生产者参数"></a>重要的生产者参数</h2><h3 id="acks"><a href="#acks" class="headerlink" title="acks"></a>acks</h3><p>指定分区中必须要有多少副本收到这条消息,生产者才认为消息成功写入</p>
<ul>
<li>acks = ‘1’。默认值 ‘1’ 。生产者发送消息后，只要消息无法写入 leader 副本，那么生产者就会收到一个错误的响应</li>
<li>acks = ‘0’。生产者发送消息之后不需要等待任何服务端的响应</li>
<li>acks = ‘-1’ 或 ‘all’ 。生产者发送消息后，需等待 ISR 中的所有副本都成功写入消息之后才能收到来自服务端的成功响应</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">properties.put(ProducerConfig.ACKS_CONFIG, <span class="string">"all"</span>);</span><br></pre></td></tr></table></figure>

<h3 id="max-request-size"><a href="#max-request-size" class="headerlink" title="max.request.size"></a>max.request.size</h3><p>用来限制生产者客户端能发送的消息的最大值，默认值为 <code>1048576B</code> ，即 <code>1MB</code>。这个参数配置需要注意 broker 端的 <code>message.max.bytes</code> 参数配置的连锁反应。</p>
<h3 id="retires-和-retry-backoff-ms"><a href="#retires-和-retry-backoff-ms" class="headerlink" title="retires 和 retry.backoff.ms"></a>retires 和 retry.backoff.ms</h3><p><code>retires</code> 生产者重试次数，默认值为 0 </p>
<p><code>retry.backoff.ms</code> 两次重试之间的间隔，默认 100</p>
<h3 id="compression-type"><a href="#compression-type" class="headerlink" title="compression.type"></a>compression.type</h3><p>自定消息的压缩方式，默认为 <code>&quot;none&quot;</code> ，默认不压缩，可选配置：<code>gizp</code> <code>snappy</code> 和 <code>lz4</code></p>
<h3 id="connections-max-idle-ms"><a href="#connections-max-idle-ms" class="headerlink" title="connections.max.idle.ms"></a>connections.max.idle.ms</h3><p>指定多久之后关闭限制的连接，默认是 544000，即 9 分钟</p>
<h3 id="linger-ms"><a href="#linger-ms" class="headerlink" title="linger.ms"></a>linger.ms</h3><p>指生产者发送 ProducerBatch 之前等待更多消息 ProduerRecord 加入 ProduerBatch 的时间，默认为 0 。生产者会在 ProduerBatch 被填满或者等待时间超过配置的值时发送出去</p>
<h3 id="receive-buffer-bytes"><a href="#receive-buffer-bytes" class="headerlink" title="receive.buffer.bytes"></a>receive.buffer.bytes</h3><p>设置 Socket 接收消息缓冲区 SO_RECBUF 的大小，默认值为 32768B ，即 128KB。如果设置为 -1 ，则使用操作系统的默认值。如果 Produer 与 kafka 处于不同的机房，则可以适当的调大这个参数值</p>
<h3 id="send-buffer-bytes"><a href="#send-buffer-bytes" class="headerlink" title="send.buffer.bytes"></a>send.buffer.bytes</h3><p>设置 Socket 发送消息缓冲区 SO_SNDBUF 的大小，默认值为 131072B，即 128KB。如果设置为 -1 ，则使用操作系统的默认值。</p>
<h3 id="request-timeout-ms"><a href="#request-timeout-ms" class="headerlink" title="request.timeout.ms"></a>request.timeout.ms</h3><p>设置 Producer 等待请求响应的最长时间，默认值为 30000ms 。请求超时之后可以选择进行重试。此配置需要比 broker 端参数 <code>replica.lag.time.max.ms</code> 的值要大，这样可以减少因客户端重试而引起的消息重复概率</p>
<h1 id="3-消费者"><a href="#3-消费者" class="headerlink" title="[3] 消费者"></a>[3] 消费者</h1><h2 id="消费者与消费组"><a href="#消费者与消费组" class="headerlink" title="消费者与消费组"></a>消费者与消费组</h2><p><strong>消费者</strong> ：在同一个消费组下的消费者只会有一个消费到消息，不在同一个消费组则每个消费组内会有一个消费者消费到消息。</p>
<p><strong>消费组</strong>：kafka 特有的概念，每个消费者都必须指定一个消费组。</p>
<ul>
<li>如果所有的消费者都属于同一个消费组，那么所有的消息都会被均衡的投递给每一个消费者，即每条消息只会被一个消费者处理，这就相当于点对点模式</li>
<li>如果所有的消费者都属于不同的消费组，那么所有的消息都会被广播给所有的消费者，即每条消息都会被所有的消费者处理，这就相当于发布/订阅模式</li>
</ul>
<h2 id="消费者客户端"><a href="#消费者客户端" class="headerlink" title="消费者客户端"></a>消费者客户端</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumerAnalysis</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> String brokerList = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"topic-demo"</span>;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> String groupId = <span class="string">"group.demo1"</span>;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> AtomicBoolean isRunning = <span class="keyword">new</span> AtomicBoolean(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Properties <span class="title">initConfig</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, 							StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, 							StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);</span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);</span><br><span class="line">        properties.put(ConsumerConfig.CLIENT_ID_CONFIG, <span class="string">"consumer.client.id.dmeo"</span>);</span><br><span class="line">        <span class="keyword">return</span> properties;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties properties = initConfig();</span><br><span class="line">        KafkaConsumer&lt;String,String&gt; consumer = <span class="keyword">new</span> KafkaConsumer(properties);</span><br><span class="line">        consumer.subscribe(Arrays.asList(topic));</span><br><span class="line">        <span class="keyword">while</span> (isRunning.get()) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records =</span><br><span class="line">                consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">            records.forEach(e -&gt; &#123;</span><br><span class="line">                System.out.println(e.toString());</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="订阅主题与分区"><a href="#订阅主题与分区" class="headerlink" title="订阅主题与分区"></a>订阅主题与分区</h2><p>主要分析 <code>KafkaConsumer</code> 类 中的方法</p>
<p><strong>subscribe</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 一种是集合类入参说明可以定义多个主题,一种是正则表达式</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener)</span> </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Collection&lt;String&gt; topics)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Pattern pattern, ConsumerRebalanceListener listener)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Pattern pattern)</span></span></span><br></pre></td></tr></table></figure>
<p><strong>assign</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定分区 注意这里是 TopicPartition</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">assign</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span></span><br><span class="line"><span class="function"><span class="comment">// 例如: </span></span></span><br><span class="line"><span class="function">consumer.<span class="title">assign</span><span class="params">(Arrays.asList(new TopicPartition(<span class="string">"topic-demo"</span>, <span class="number">0</span>)</span>))</span>;</span><br></pre></td></tr></table></figure>

<p>如果不知道主题下有多少分区 可使用 <code>partitionFor</code>  查看该主题下有多少分区</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> List&lt;PartitionInfo&gt; <span class="title">partitionsFor</span><span class="params">(String topic)</span></span></span><br></pre></td></tr></table></figure>

<p><strong>unSubscibe</strong></p>
<p>取消订阅主题</p>
<h2 id="反序列化"><a href="#反序列化" class="headerlink" title="反序列化"></a>反序列化</h2><p>和 生产者 的序列化使用模式一致，继承 <code>Deserializer</code></p>
<h2 id="消息消费"><a href="#消息消费" class="headerlink" title="消息消费"></a>消息消费</h2><p>消息的消费一般有两种模式：推 和 拉 模式。推模式是服务端主动将消息推给消费者，而拉模式是消费者主动向服务端发起请求来拉取消息。 kafka 采用的是拉模式</p>
<p>调用 <code>poll()</code> 方法拉取消息</p>
<h2 id="消息位移"><a href="#消息位移" class="headerlink" title="消息位移"></a>消息位移</h2><h3 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h3><p>kafka 分区中的每条消息都有唯一 offset ，用于表示消息在分区中对应的位置。利用这个 offset 就可以知道消息在分区的哪个位置</p>
<p>每次调用 <code>poll()</code>方法，它返回的是还没被消费过消息集，要做到这一点就需要记录上一次的消费位移 offset</p>
<h3 id="自动位移提交"><a href="#自动位移提交" class="headerlink" title="自动位移提交"></a>自动位移提交</h3><p>在 kafka 中默认自动提交消费位移，由客户端参数<code>enable.auto.commit</code> 配置值为 <code>true</code> 。在 <code>poll()</code> 方法内部，实现了自动位移提交动作的逻辑</p>
<p>自动提交消费位移的方式非常方便，我们不需要写额外的同步位移代码。在默认的自动提交方式 消费者是<strong>每隔 5 秒</strong> 拉取每个分区最大的消息位移进行提交。假设刚刚提交完一次消费位移，然后拉取一批消息进行消费时，消费者奔溃了，恢复后又会从上一次位移提交的地方重新开始消费，这就照成了 <strong>重复消费</strong> 的现象</p>
<h3 id="手动位移提交"><a href="#手动位移提交" class="headerlink" title="手动位移提交"></a>手动位移提交</h3><p><code>enable.auto.commit</code> 配置为 <code>false</code></p>
<p>手动提交细分为两种同步提交和异步提交，即 commitSync 和 commitAsync</p>
<h4 id="commitSync-同步提交"><a href="#commitSync-同步提交" class="headerlink" title="commitSync 同步提交"></a>commitSync 同步提交</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (isRunning.get()) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">    <span class="keyword">for</span>(ConsumerRecord&lt;String,String&gt; record : records)&#123;</span><br><span class="line">        <span class="comment">// do some logical processing</span></span><br><span class="line">    &#125;</span><br><span class="line">    consumer.commitSync();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>更加细粒度同步提交消费位移</strong></p>
<p>使用 <code>commitSync(final Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets)</code> 方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//每条消息都细粒度的同步消费位移，性能极低</span></span><br><span class="line"><span class="keyword">while</span> (isRunning.get()) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">    records.forEach(record -&gt; &#123;</span><br><span class="line">        System.out.println(record.toString());</span><br><span class="line">        <span class="keyword">long</span> offset = record.offset();</span><br><span class="line">        TopicPartition partition = <span class="keyword">new</span> TopicPartition(record.topic(), 																	record.partition());</span><br><span class="line">        consumer.commitSync(Collections.singletonMap(partition, <span class="keyword">new</span> 														OffsetAndMetadata(offset + <span class="number">1</span>)));</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 通常使用 按分区粒度同步消费位移</span></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">    records.partitions().forEach(partition -&gt; &#123;</span><br><span class="line">        List&lt;ConsumerRecord&lt;String, String&gt;&gt; partitionRecords = 	 																 records.records(partition);</span><br><span class="line">        partitionRecords.forEach(record -&gt; &#123;</span><br><span class="line">            <span class="comment">// do some logical proccessing.</span></span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 定位最后一条消息的 offset</span></span><br><span class="line">        <span class="keyword">long</span> lastConsumeOfset = partitionRecords.get(partitionRecords.size() -   														 <span class="number">1</span>).offset();</span><br><span class="line">        consumer.commitSync(Collections.singletonMap(partition, <span class="keyword">new</span> 												OffsetAndMetadata(lastConsumeOfset + <span class="number">1</span>)));</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="commitAsync-异步提交"><a href="#commitAsync-异步提交" class="headerlink" title="commitAsync 异步提交"></a>commitAsync 异步提交</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">consumer.commitAsync((offsets, exception) -&gt; &#123;</span><br><span class="line">    <span class="keyword">if</span> (exception == <span class="keyword">null</span>) &#123;</span><br><span class="line">        System.out.println(offsets);</span><br><span class="line">    &#125; <span class="keyword">else</span></span><br><span class="line">        System.out.println(<span class="string">"fail to commit offsets "</span> + offsets + exception);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>



<h2 id="控制或关闭消费"><a href="#控制或关闭消费" class="headerlink" title="控制或关闭消费"></a>控制或关闭消费</h2><p><code>kafkaConsumer</code> 中使用 <code>pause()</code> 和 <code>resume()</code> 来暂停对某些分区在拉取操作时返回数据给客户端 和 恢复某些分区向客户端返回数据的操作</p>
<h2 id="指定位移消费"><a href="#指定位移消费" class="headerlink" title="指定位移消费"></a>指定位移消费</h2><p>当一个新的消费组建立的时候，它没有可以查找的消费位移，或者消费组内的一个新的消费者订阅了新的主题，它也没有可以查找的消费位移。</p>
<p>在 kafka 中 如果消费者找不到所记录的消费位移时，就会根据消费者客户端参数 <code>auto.offset.reset</code> 的配置来决定从何处开始消费。</p>
<p><code>auto.offset.reset</code></p>
<ul>
<li><p>latest（默认） 表示从分区末尾开始消费消息</p>
</li>
<li><p>earliest 表示从 0 开始消费</p>
</li>
<li><p>none 不从末尾也不从头消费，而是报错 <code>NoOffsetForPartitionException</code> 异常</p>
</li>
</ul>
<h3 id="seek"><a href="#seek" class="headerlink" title="seek()"></a>seek()</h3><p>当想自由的细粒度操作时，使用 <code>KafkaConsumer</code> 中的 <code>seek()</code> 方法，指定从哪个位置开始消费</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seekTest</span><span class="params">()</span></span>&#123;</span><br><span class="line">    KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br><span class="line">    consumer.subscribe(Arrays.asList(topic));</span><br><span class="line">    consumer.poll(Duration.ofMillis(<span class="number">10000</span>));<span class="comment">// 随意设置 10 秒 等待时间</span></span><br><span class="line">    Set&lt;TopicPartition&gt; assignment = consumer.assignment();</span><br><span class="line">    <span class="keyword">for</span> (TopicPartition tp :assignment)&#123;</span><br><span class="line">        consumer.seek(tp, <span class="number">10</span>);<span class="comment">// 将在 offset 10 的位置开始消费</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (isRunning.get())&#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records =    					 											consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">        records.forEach(e -&gt; System.out.println(e.toString()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>优化等待时间 10 秒 照成的浪费</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">    Set&lt;TopicPartition&gt; assignment = <span class="keyword">new</span> HashSet();</span><br><span class="line">    <span class="keyword">while</span> (assignment.size() == <span class="number">0</span>) &#123;</span><br><span class="line">        consumer.poll(Duration.ofMillis(<span class="number">100</span>)); <span class="comment">// 设置0.1 秒 重试 直到拉取完成</span></span><br><span class="line">        assignment = consumer.assignment();</span><br><span class="line">    &#125;</span><br><span class="line">	assignment.forEach(tp -&gt; consumer.seek(tp, <span class="number">10</span>));</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>使用 <code>seek()</code> 从分区末尾消费，<code>endOffSets</code> 获取每个分区最后一个 offset 位置。对应的还有 <code>beginningOffsets()</code>  方法获取初始位置</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="comment">// 从末尾开始消费</span></span><br><span class="line">Map&lt;TopicPartition, Long&gt; offsets = consumer.endOffsets(assignment);</span><br><span class="line">assignment.forEach(tp -&gt; consumer.seek(tp, offsets.get(tp)));</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// kafkaConsumer 直接就提供了 从头 从未消费方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seekToBeginning</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seekToEnd</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span></span><br></pre></td></tr></table></figure>

<p>指定时间消费，如下是指定从 昨天的当前时间开始消费</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">Map&lt;TopicPartition, Long&gt; timestampToSearch = <span class="keyword">new</span> HashMap();</span><br><span class="line">assignment.forEach(tp -&gt; timestampToSearch.put(tp, System.currentTimeMillis() - <span class="number">1</span> * <span class="number">24</span> * <span class="number">3600</span> * <span class="number">1000</span>));<span class="comment">// 从指定的时间开始消费</span></span><br><span class="line">Map&lt;TopicPartition, OffsetAndTimestamp&gt; offsets = consumer.offsetsForTimes(timestampToSearch);</span><br><span class="line">assignment.forEach(tp -&gt; &#123;</span><br><span class="line">    OffsetAndTimestamp offsetAndTimestamp = offsets.get(tp);</span><br><span class="line">    <span class="keyword">if</span> (offsetAndTimestamp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">        consumer.seek(tp, offsetAndTimestamp.offset());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">...</span><br></pre></td></tr></table></figure>



<h2 id="再均衡"><a href="#再均衡" class="headerlink" title="再均衡"></a>再均衡</h2><p>指分区的所属权从一个消费者转移到另一个消费者的行为，它为消费组具备高可用性和伸缩性提供保障。在均衡发生期间，消费组内的消费者不可用。</p>
<p><code>onPartitionsRevoked</code> 该方法会在再均衡开始之前和消费者停止读取消息之后调用</p>
<p><code>onPartitionsAssigned</code> 该方法会在重新分配分区之后和消费者开始读取消息之前被调用</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">consumer.subscribe(Arrays.asList(topic), <span class="keyword">new</span> ConsumerRebalanceListener() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">		<span class="comment">// 可以存储 offset 到 数据库</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">    	<span class="comment">// 可以从数据库中取出 offset  配合 seek()</span></span><br><span class="line">        partitions.foreach(tp -&gt; consumer.seek(tp, getOffsetFromDB(tp)));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>



<h2 id="消费者拦截器"><a href="#消费者拦截器" class="headerlink" title="消费者拦截器"></a>消费者拦截器</h2><p>消费者自定义实现 <code>ConsumerInterceptor</code> 接口</p>
<ul>
<li>onConsumer() KafkaConsumer 会在 poll() 方法返回值之前调用</li>
<li>onCommit() KafkaConsumer 会在提交完消费位移之后调用拦截器</li>
</ul>
<p>拦截器可以实现对消息的验证过滤</p>
<h2 id="重要的消费者参数"><a href="#重要的消费者参数" class="headerlink" title="重要的消费者参数"></a>重要的消费者参数</h2><p>百度吧</p>
<h2 id="使用多线程提高消费能力"><a href="#使用多线程提高消费能力" class="headerlink" title="使用多线程提高消费能力"></a>使用多线程提高消费能力</h2><ul>
<li>使用 <code>SynchronousQueue</code> 一进一出队列，避免队列里缓冲数据，这样在系统异常关闭时，就能排除因为阻塞队列丢失消息的可能</li>
<li>使用 <code>ThreadPoolExecutor.CallerRunsPolicy()</code> 饱和策略，使得多线程处理不过来的时候，能够阻塞在 kafka 的消费线程上</li>
<li>相关参数配置</li>
</ul>
<p><strong>max.poll.records</strong></p>
<p>调用一次poll，返回的最大条数。这个值设置的大，那么处理的就慢，很容易超出<code>max.poll.interval.ms</code>的值（默认5分钟），造成消费者的离线。在耗时非常大的消费中，是需要特别注意的。</p>
<p><strong>enable.auto.commit</strong></p>
<p>是否开启自动提交（offset）如果开启，consumer已经消费的offset信息将会间歇性的提交到kafka中（持久保存）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer(properties);</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">        <span class="keyword">while</span> (isRunning.get()) &#123;</span><br><span class="line">            consumer.subscribe(Collections.singletonList(topic));</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records =   	       														consumer.poll(Duration.ofMillis(<span class="number">100</span>));</span><br><span class="line">            records.forEach(record -&gt; &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    queue.put(record);</span><br><span class="line">                    consumer.commitSync();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;).start();</span><br><span class="line">    initPipelineThread();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">processRecordItem</span><span class="params">(ConsumerRecord&lt;String, String&gt; record)</span> </span>&#123;</span><br><span class="line">    System.out.println(Thread.currentThread().getName() + record.toString());</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">initPipelineThread</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (isRunning.get()) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">final</span> ConsumerRecord&lt;String, String&gt; record = queue.poll(<span class="number">5</span>,  															TimeUnit.MINUTES);</span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">null</span> != record)</span><br><span class="line">                executor.submit(() -&gt; &#123;</span><br><span class="line">                    processRecordItem(record);</span><br><span class="line">                &#125;);	</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里是测试用例，<strong>实际项目中用法可参考文章</strong>：<br><a href="https://segmentfault.com/a/1190000018640106" target="_blank" rel="noopener">使用多线程增加kafka消费能力</a></p>
<h1 id="4-主题与分区"><a href="#4-主题与分区" class="headerlink" title="[4] 主题与分区"></a>[4] 主题与分区</h1><p>​    主题座位消息的归类，可以在媳妇为一个或多个分区，分区也可以看作对消息的二次归类。分区的划分不仅为 kafka 提供了可伸缩性，水平扩展的功能，还通过多副本机制来为 kafka 提供数据冗余以提高数据可靠性</p>
<h2 id="创建主题"><a href="#创建主题" class="headerlink" title="创建主题"></a>创建主题</h2><p>broker端配置参数:</p>
<p><strong>auto.create.topic.enalbe</strong>  默认为 true</p>
<p>当生产者向一个尚未创建的主题发送消息时，会自动创建一个分区数为 <strong>num.partitions</strong>(默认值1) 、副本因子为 <strong>default.replication.factor</strong>(默认值1) 的主题</p>
<p>当消费者向一个未知主题读取消息时，或者当任意一个客户端向未知主题发送元数据请求时，都会按照配置参数 <strong>num.partitions</strong>、<strong>default.replication.factor</strong> 的值来创建一个相应的主题</p>
<p>使用 kafka-topics,.sh 脚本来创建主题实际上是调用了 kafka.admin.TopicCommand 类</p>
<p>创建一个分区数为 4 、副本因子为 2 的主题 demo</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin\kafka-topics.sh --zookeeper localhost:2181/kafka --create --topic demo --partitions 4 --replication-factor 2</span><br></pre></td></tr></table></figure>

<p>执行完脚本后，kafka 会在 <code>log.dir</code> 或 <code>log/dirs</code> 参数所配置的目录下创建相应的主题分区，默认目录为<code>/tmp/kafka-logs</code>， 这取决于你的 <code>server.properties</code> 配置</p>
<p>根据 kafka 的日志文件查看该节点下创建的主题分区</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ls -al /opt/kafka/logs/ | grep topic-create</span><br><span class="line"></span><br><span class="line">drwxr-xr-x.  2 root root  141 12月 28 19:25 --topic-create-1</span><br><span class="line">drwxr-xr-x.  2 root root  141 12月 28 19:25 --topic-create-2</span><br><span class="line">drwxr-xr-x.  2 root root  141 12月 28 19:25 --topic-create-3</span><br></pre></td></tr></table></figure>





<p>根据 zookeeper 客户端获取——当创建一个主题会在 zookeeper 的 <code>/brokers/topics</code> 目录下创建同名的实节点</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get /brokers/topics/demo</span><br><span class="line">&#123;<span class="string">"version"</span>:1,<span class="string">"partitions"</span>:&#123;<span class="string">"2"</span>:[1,2],<span class="string">"1"</span>:[0,1],<span class="string">"3"</span>:[2,1],<span class="string">"0"</span>:[2,0]&#125;&#125;</span><br></pre></td></tr></table></figure>

<p>返回的数据 —— “2” : [1,2] 表示分区 2 分配了 2 个副本，分别在 brokerId 为 1 和 2 的 broker 节点中</p>
<p><img src="../images/1577082082(1).jpg" alt="关系图"></p>
<h3 id="查看主题"><a href="#查看主题" class="headerlink" title="查看主题"></a>查看主题</h3><p>​    通过 list 指令可以查看当前所有可用的主题</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin\kafka-topics.sh --zookeeper localhost:2181/kafka -list</span><br></pre></td></tr></table></figure>

<p>​    通过 describe 指令查看所有主题详细信息。–topic 指定主题分析</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin\kafka-topics.sh --zookeeper localhost:<span class="number">2181</span>/kafka --descibe --topic demo</span><br></pre></td></tr></table></figure>



<h3 id="修改主题"><a href="#修改主题" class="headerlink" title="修改主题"></a>修改主题</h3><p>当主题被创建之后，可以修改分区个数、修改配置等，由 kafka-topics.sh 脚本中的 alter 指令提供</p>
<p>增加主题demo的分区数</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181/kafka --alter --topic demo --partitions 3</span><br></pre></td></tr></table></figure>

<p>修改分区数会影响既定的消息顺序，当主题中的消息包含 key 时，根据 key 计算分区的行为就会收到影响</p>
<p>建议在一开始就设置好分区数量，避免以后对其进行调整。</p>
<p>目前 kafka 只支持增加分区数而不支持减少分区数</p>
<p>修改主题配置<code>max.messgae.bytes=20000</code></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181/kafka --alter --topic demo --config max.messgae.bytes=20000</span><br></pre></td></tr></table></figure>



<h3 id="删除主题"><a href="#删除主题" class="headerlink" title="删除主题"></a>删除主题</h3><p>使用 kafka-topics.sh 脚本中的 delete 指令删除主题demo</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181/kafka --delete --topic demo</span><br></pre></td></tr></table></figure>

<p>手动方式删除主题</p>
<p>主题中的元数据存储在 zookeeper 中的 /broker/topics 和 /config/topics 路径下</p>
<p>主题中的消息存储在 log.dir 或 log.dirs 配置的路径下</p>
<ul>
<li>1 删除 zookeeper 中的节点  /config/topics/demo</li>
<li>rmr /config/topics/demo</li>
<li>2 删除 zookeeper 中的节点 /brokers/topics/demo</li>
<li>delete /brokers/topics/demo</li>
<li>3 删除集群中所有与主题 topics-delete 有关的文件</li>
<li>集群中的各个 broker 节点中执行 rm -rf /tmp/kafka-logs/demo</li>
</ul>
<h3 id="使用-kafkaAdminClient"><a href="#使用-kafkaAdminClient" class="headerlink" title="使用 kafkaAdminClient"></a>使用 kafkaAdminClient</h3><p>创建主题</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> String brokerList = <span class="string">"localhost:9092"</span>;</span><br><span class="line"><span class="keyword">static</span> String topic = <span class="string">"topic-admin"</span>;</span><br><span class="line"><span class="keyword">static</span> Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line">    properties.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);</span><br><span class="line">    properties.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, <span class="number">30000</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">create</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    AdminClient client = AdminClient.create(properties);</span><br><span class="line">    NewTopic newTopic = <span class="keyword">new</span> NewTopic(topic, <span class="number">4</span>, (<span class="keyword">short</span>) <span class="number">1</span>);</span><br><span class="line">    CreateTopicsResult result = client.createTopics(Collections.singleton(newTopic));</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        result.all().get();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    client.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>分析主题</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">describeTopic</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    AdminClient client = AdminClient.create(properties);</span><br><span class="line">    DescribeTopicsResult describeTopicsResult = client.describeTopics(Collections.singleton(topic));</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        Map&lt;String, TopicDescription&gt; stringTopicDescriptionMap =</span><br><span class="line">            describeTopicsResult.all().get();</span><br><span class="line">        System.out.println(stringTopicDescriptionMap);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>更多操作请百度</p>
<h1 id="5-日志存储"><a href="#5-日志存储" class="headerlink" title="[5] 日志存储"></a>[5] 日志存储</h1><p>​    kafka 的消息 称为日志， 日志持久化在磁盘上</p>
<h2 id="文件目录布局"><a href="#文件目录布局" class="headerlink" title="文件目录布局"></a>文件目录布局</h2><p>​        主题可分为多个分区，每个分区对应一个日志 (log) ，为了防止 Log 过大，又切分多个 LogSegment  。Log 在物理上只以文件夹的形式存储，而每个 LogSegment 对应于磁盘上的一个日志文件和两个索引文件，以及可能的其他文件。</p>
<img url="../image/kafka日志关系.PNG"/>

<p><img src=".%5Cimages%5Ckafka%E6%97%A5%E5%BF%97%E5%85%B3%E7%B3%BB.PNG" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.png"
                alt="huanghao" />
            
              <p class="site-author-name" itemprop="name">huanghao</p>
              <p class="site-description motion-element" itemprop="description">this is my blog</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">huanghao</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("cbIWeHuG3gM3D2VqbCQ850mw-gzGzoHsz", "6xzwPfqRMvcSCqgL34v32WaB");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
